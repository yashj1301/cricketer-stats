# ğŸ Cricketer Stats Analytics Pipeline

A modular Python project to scrape, process, and analyze detailed cricket player statistics from ESPN CricInfo. The data is stored in AWS S3 and visualized using Power BI dashboards.

---

## ğŸ” Overview

This project builds an end-to-end data pipeline that:

* Scrapes player-level data (batting, bowling, fielding, all-rounder stats, and personal info)
* Transforms and standardizes the data
* Aggregates it into master datasets
* Uploads it to AWS S3 for dashboarding in Power BI or Tableau

The pipeline is structured using modular Python scripts and classes, with extensibility for containerization and orchestration.

---

## ğŸ”€ Tech Stack

| Tool         | Purpose                    |
| ------------ | -------------------------- |
| **Python**   | Core programming language  |
| **Selenium** | Web scraping from CricInfo |
| **pandas**   | Data transformation        |
| **boto3**    | Interacting with AWS S3    |
| **Power BI** | Dashboard visualization    |

---

## ğŸ” Workflow

1. **Scrape data** using Selenium for a specific player.
2. **Transform data** into clean, analysis-ready format.
3. **Aggregate** data across multiple players into master datasets.
4. **Upload to AWS S3** in both raw and transformed forms.
5. **Visualize** insights using Power BI.

---

## ğŸ“‚ Project Structure

```
cricketer-stats/
â”œâ”€â”€ research lab/                 # Exploratory notebooks or R&D scripts
â”‚   â”œâ”€â”€ EDA.ipynb                 # EDA Notebook
â”‚   â”œâ”€â”€ check.ipynv               # notebook for experimentation
â”œâ”€â”€ scripts/                      # Python module folder with core logic
â”‚   â”œâ”€â”€ scraper/                  # Web scraping module
â”‚   â”œâ”€â”€ transformer/              # Data cleaning and formatting module
â”‚   |â”€â”€ loader/                   # S3 upload/download logic
â”‚   |â”€â”€ aggregator/               # aggregation logic to generate master dataframes
â”œâ”€â”€ scripts.egg-info/             # Auto-generated metadata for Python packaging
â”œâ”€â”€ tests/                        # Test scripts for modules
â”‚   â”œâ”€â”€ aggregator_test.py
â”‚   â”œâ”€â”€ scraper_test.py
â”‚   â””â”€â”€ transformer_test.py
â”œâ”€â”€ visuals/                      # Power BI (.pbix) and design elements
â”œâ”€â”€ .env                          # AWS credentials and other environment setup
â”œâ”€â”€ .gitignore
â”œâ”€â”€ extras.md                     # Feature backlog and future plans
â”œâ”€â”€ README.md                     # Project documentation
â”œâ”€â”€ requirements.txt              # List of Python dependencies
â”œâ”€â”€ setup.py                      # Package setup file for pip installation
â”œâ”€â”€ test.py                       # Optional test runner script
â”œâ”€â”€ workflow.md                   # Workflow explanation
```

---

## ğŸ“Œ Key Features

* ğŸ” Scrapes detailed stats: **batting, bowling, fielding, allround**, and **player profile**
* â™»ï¸ Clean modular classes: **ScrapeData, TransformData, LoadData, Aggregator**
* â˜ï¸ Uses AWS S3 as the cloud data store
* ğŸ“Š Output ready for Power BI or Tableau
* ğŸ§± Code structured for easy scaling and future automation

---

## ğŸš€ Setup Instructions

1. Clone this repository:

   ```bash
   git clone https://github.com/your-username/cricketer-stats.git
   cd cricketer-stats
   ```
2. Create and configure a `.env` file with your AWS credentials:

   ```env
   AWS_ACCESS_KEY_ID=your_key
   AWS_SECRET_ACCESS_KEY=your_secret
   AWS_DEFAULT_REGION=ap-south-1
   ```
3. Install project dependencies:

   ```bash
   pip install -r requirements.txt
   ```
4. Install the project locally as a package:

   ```bash
   pip install -e .
   ```
5. Run any of the test scripts:

   ```bash
   python tests/aggregator_test.py
   ```

   > âš ï¸ Make sure to set `player_name` and `bucket_name` before running the tests.

---

## ğŸ›£ Roadmap & Extras

See `extras.txt` for upcoming enhancements, including:

* Dynamic ground info scraping
* Incremental loading logic
* Secret manager integration
* Docker + Airflow orchestration
* Custom exception handling

---

## ğŸ“– Using as a Local Python Package

This project is structured as an installable package using `setup.py`. This allows you to import the core modules (scraper, loader, transformer, aggregator) anywhere in your system.

### Installation:

From the project root, run:

```bash
pip install -e .
```

### Example Usage:

```python
from scraper import ScrapeData
from transformer import TransformData
```

This makes testing and modular development cleaner, especially across notebooks and scripts.

---

## ğŸ“– For Reference

See `workflow.md` for a high-level overview of the entire ETL-to-dashboard pipeline.

---
